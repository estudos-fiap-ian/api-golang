name: Deploy to EKS

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  EKS_CLUSTER_NAME: grupo-275-cluster-ian
  ECR_REPOSITORY: golunch-api
  IMAGE_TAG: ${{ github.sha }}

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Create ECR repository if it doesn't exist
      run: |
        aws ecr describe-repositories --repository-names $ECR_REPOSITORY --region $AWS_REGION || \
        aws ecr create-repository --repository-name $ECR_REPOSITORY --region $AWS_REGION

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: Build, tag, and push image to Amazon ECR
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
      run: |
        # Build the Docker image
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:latest .

        # Push both tagged and latest images
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

    - name: Install EBS CSI Driver and Setup Storage
      run: |
        # Check if EBS CSI driver addon is installed
        if ! aws eks describe-addon --cluster-name ${{ env.EKS_CLUSTER_NAME }} --addon-name aws-ebs-csi-driver --region ${{ env.AWS_REGION }} 2>/dev/null; then
          echo "Installing EBS CSI driver addon..."
          aws eks create-addon \
            --cluster-name ${{ env.EKS_CLUSTER_NAME }} \
            --addon-name aws-ebs-csi-driver \
            --region ${{ env.AWS_REGION }} \
            --resolve-conflicts OVERWRITE || echo "EBS CSI driver may already be installed via other method"

          # Wait longer for addon to be fully ready
          echo "Waiting for EBS CSI driver to be ready..."
          sleep 60
        else
          echo "EBS CSI driver addon already exists"
        fi

        # Wait for CSI driver pods to be running
        kubectl wait --for=condition=Ready pod -l app=ebs-csi-controller -n kube-system --timeout=300s || echo "CSI controller pods not ready, continuing..."
        kubectl wait --for=condition=Ready pod -l app=ebs-csi-node -n kube-system --timeout=300s || echo "CSI node pods not ready, continuing..."

        # Handle storage class properly
        CURRENT_PROVISIONER=$(kubectl get storageclass gp2 -o jsonpath='{.provisioner}' 2>/dev/null || echo "")
        if [ "$CURRENT_PROVISIONER" != "ebs.csi.aws.com" ]; then
          echo "Deleting old storage class with provisioner: $CURRENT_PROVISIONER"
          kubectl delete storageclass gp2 --ignore-not-found=true

          # Create new storage class with correct provisioner
          kubectl apply -f - <<EOF
        apiVersion: storage.k8s.io/v1
        kind: StorageClass
        metadata:
          name: gp2
          annotations:
            storageclass.kubernetes.io/is-default-class: "true"
        provisioner: ebs.csi.aws.com
        parameters:
          type: gp2
          fsType: ext4
        allowVolumeExpansion: true
        volumeBindingMode: WaitForFirstConsumer
        EOF
        else
          echo "Storage class gp2 already exists with correct provisioner"
        fi

        # List available storage classes
        echo "Available storage classes:"
        kubectl get storageclass

    - name: Deploy to EKS
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
      run: |
        # Replace image in deployment manifest
        sed -i "s|lucasonofre/golunch:v1|$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG|g" k8s/app-deployment.yaml

        # Apply Kubernetes manifests
        kubectl apply -f k8s/secrets.yaml
        kubectl apply -f k8s/configmap.yaml

        # Handle PVC recreation if storage class changed
        if kubectl get pvc uploads-pvc 2>/dev/null; then
          CURRENT_SC=$(kubectl get pvc uploads-pvc -o jsonpath='{.spec.storageClassName}')
          if [ "$CURRENT_SC" != "gp2" ]; then
            echo "Deleting existing PVC with incorrect storage class: $CURRENT_SC"
            # Delete deployment first to release PVC
            kubectl delete deployment go-web-api --ignore-not-found=true --timeout=60s

            # Wait for pods to terminate
            kubectl wait --for=delete pod -l app=go-web-api --timeout=120s || echo "Pods still terminating, continuing..."

            # Now delete PVC
            kubectl delete pvc uploads-pvc --wait=true --timeout=60s
          fi
        fi

        # Skip PVC for now and use simpler storage approach
        echo "Using deployment without persistent storage for faster deployment"
        kubectl delete pvc uploads-pvc --ignore-not-found=true --force --grace-period=0
        DEPLOYMENT_FILE="k8s/app-deployment-no-pvc.yaml"

        # Future: uncomment below to try PVC first
        # kubectl apply -f k8s/app-uploads-pvc.yaml
        # if kubectl wait --for=condition=Bound pvc/uploads-pvc --timeout=30s; then
        #   DEPLOYMENT_FILE="k8s/app-deployment.yaml"
        # else
        #   kubectl delete pvc uploads-pvc --ignore-not-found=true
        #   DEPLOYMENT_FILE="k8s/app-deployment-no-pvc.yaml"
        # fi

        # Apply other resources - using deployment instead of statefulset to avoid PVC issues
        kubectl delete statefulset postgres --ignore-not-found=true
        kubectl apply -f k8s/postgre-deployment-no-pvc.yaml
        kubectl apply -f k8s/postgre-service.yaml

        # Wait a moment before deploying the app
        sleep 10
        kubectl apply -f $DEPLOYMENT_FILE

        # Apply the main NodePort service for NLB integration
        kubectl apply -f k8s/app-service.yaml

        # Apply the optimized NLB service
        kubectl apply -f k8s/app-service-nlb.yaml

        # Apply healthcheck service
        kubectl apply -f k8s/healthcheck-service.yaml

        # Apply ingress and HPA
        kubectl apply -f k8s/ingress.yaml
        kubectl apply -f k8s/hpa.yaml

        # Clean up any existing LoadBalancer service that conflicts with Terraform NLB
        kubectl delete service go-web-api-service-lb --ignore-not-found=true

        # Wait for deployment to be ready
        kubectl rollout status deployment/go-web-api --timeout=300s

    - name: Verify deployment
      run: |
        kubectl get pods -l app=go-web-api
        kubectl get services
        kubectl describe service go-web-api-service
        kubectl describe service go-web-api-service-nlb
        echo "Checking NodePort service for NLB integration..."
        kubectl get service go-web-api-service -o jsonpath='{.spec.ports[0].nodePort}'

    - name: Get Load Balancer URLs and Configuration
      run: |
        echo "Waiting for Load Balancer to be ready..."
        sleep 60

        # Note: Using Terraform-managed NLB instead of Kubernetes LoadBalancer service
        # NLB URL will be provided by terraform-infra outputs
        echo "Using Terraform-managed NLB for API Gateway integration"

        # Get Application Load Balancer URL from Ingress
        ALB_URL=$(kubectl get ingress go-web-api-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")

        # Get NodePort service details
        NODE_PORT=$(kubectl get service go-web-api-service -o jsonpath='{.spec.ports[0].nodePort}')

        echo "=== Deployment URLs ==="
        echo "üîó Terraform-managed NLB: Configured via terraform-infra"
        echo "   API Gateway routes through VPC Link to this NLB"
        echo "   Health Check: /ping endpoint on NodePort 30080"

        if [ -n "$ALB_URL" ]; then
          echo "üîó Application Load Balancer: http://$ALB_URL"
          echo "   Health Check: http://$ALB_URL/ping"
        fi

        echo "üîó NodePort Service: NodePort $NODE_PORT (should be 30080)"
        echo "   Direct EKS access for troubleshooting"
        echo ""

        # Output for GitHub Actions environment
        echo "ALB_URL=$ALB_URL" >> $GITHUB_ENV
        echo "NODE_PORT=$NODE_PORT" >> $GITHUB_ENV

        # Verify NodePort is correctly set to 30080
        if [ "$NODE_PORT" = "30080" ]; then
          echo "‚úÖ NodePort correctly set to 30080 for Terraform NLB integration"
        else
          echo "‚ö†Ô∏è  WARNING: NodePort is $NODE_PORT, expected 30080 for Terraform NLB"
        fi

  notify-success:
    needs: build-and-deploy
    runs-on: ubuntu-latest
    if: success()

    steps:
    - name: Notify deployment success
      run: |
        echo "‚úÖ Deployment to EKS completed successfully!"
        echo "üìä Image: ${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}"
        echo "üè∑Ô∏è  Commit: ${{ github.sha }}"

  notify-failure:
    needs: build-and-deploy
    runs-on: ubuntu-latest
    if: failure()

    steps:
    - name: Notify deployment failure
      run: |
        echo "‚ùå Deployment to EKS failed!"
        echo "üè∑Ô∏è  Commit: ${{ github.sha }}"
        exit 1